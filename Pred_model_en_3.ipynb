{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import where\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "import scipy\n",
    "import sklearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn import linear_model\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\n",
    "from scipy import stats\n",
    "from imblearn.under_sampling import NearMiss\n",
    "import smote_variants as sv\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm10 = pd.read_csv(os.path.join(dataset_dir, 'pm10_testing.csv'))\n",
    "df_bc = pd.read_csv(os.path.join(dataset_dir, 'bc_testing.csv'))\n",
    "df_ozone = pd.read_csv(os.path.join(dataset_dir, 'ozone_testing.csv'))\n",
    "df_no = pd.read_csv(os.path.join(dataset_dir, 'no_testing.csv'))\n",
    "df_uv = pd.read_csv(os.path.join(dataset_dir, 'uv_testing.csv'))\n",
    "df_pm2 = pd.read_csv(os.path.join(dataset_dir, 'pm2_testing.csv'))\n",
    "df_temp = pd.read_csv(os.path.join(dataset_dir, 'temp_testing.csv'))\n",
    "df_wind = pd.read_csv(os.path.join(dataset_dir, 'wind_testing.csv'))\n",
    "data_hosp = pd.read_csv(os.path.join(dataset_dir, 'hosp_withoutcbu_auck.csv'))\n",
    "df_RH = pd.read_csv(os.path.join(dataset_dir, 'RH_testing.csv'))\n",
    "df_pres = pd.read_csv(os.path.join(dataset_dir, 'pres_testing.csv'))\n",
    "df_no2 = pd.read_csv(os.path.join(dataset_dir, 'no2_testing.csv'))\n",
    "df_prec = pd.read_csv(os.path.join(dataset_dir, 'prec_testing.csv'))\n",
    "df_so = pd.read_csv(os.path.join(dataset_dir, 'so_testing.csv'))\n",
    "df_trends = pd.read_csv(os.path.join(dataset_dir, 'goo_trends.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pm10 = pd.read_csv(\"H:/asthma/data/pm10_testing.csv\")\n",
    "#df_bc = pd.read_csv(\"H:/asthma/data/bc_testing.csv\")\n",
    "#df_ozone = pd.read_csv(\"H:/asthma/data/ozone_testing.csv\")\n",
    "#df_no = pd.read_csv(\"H:/asthma/data/no_testing.csv\")\n",
    "#df_uv = pd.read_csv(\"H:/asthma/data/uv_testing.csv\")\n",
    "#df_pm2 = pd.read_csv(\"H:/asthma/data/pm2_testing.csv\")\n",
    "#df_temp = pd.read_csv(\"H:/asthma/data/temp_testing.csv\")\n",
    "#df_wind = pd.read_csv(\"H:/asthma/data/wind_testing.csv\")\n",
    "#data_hosp = pd.read_csv(\"H:/asthma/data/hosp_withoutcbu_auck.csv\")\n",
    "#df_RH = pd.read_csv(\"H:/asthma/data/RH_testing.csv\")\n",
    "#df_pres = pd.read_csv(\"H:/asthma/data/pres_testing.csv\")\n",
    "#df_no2 = pd.read_csv(\"H:/asthma/data/no2_testing.csv\")\n",
    "#df_prec = pd.read_csv(\"H:/asthma/data/prec_testing.csv\")\n",
    "#df_so = pd.read_csv(\"H:/asthma/data/so_testing.csv\")\n",
    "#df_trends = pd.read_csv(\"H:/asthma/data/goo_trends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_ozone\n",
    "right = df_bc\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_no\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_uv\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_pm2\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_pm10\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_temp\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_wind\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_RH\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_prec\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_pres\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_no2\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = df_trends\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hosp.rename(columns = {'Admit Date' : 'Date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_f\n",
    "right = data_hosp\n",
    "#left['Date(NZST)'] = pd.to_datetime(left[\"Date(NZST)\"])\n",
    "#right['Date(NZST)'] = pd.to_datetime(right[\"Date(NZST)\"])\n",
    "df_f = pd.merge(left, right, on=[\"Date\"], how= \"outer\", validate=\"one_to_one\").sort_values(by = \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"oz\" + \" \" + str(pearsonr(df_f['capped_ozone'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"bc370\" + \" \" + str(pearsonr(df_f['capped_bc(370)'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"bc880\" + \" \" + str(pearsonr(df_f['capped_bc(880)'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"wind\" + \" \" + str(pearsonr(df_f['capped_wind'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"uvi\" + \" \" + str(pearsonr(df_f['UVI'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"no2\" + \" \" + str(pearsonr(df_f['avg_no_y'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"nox\" + \" \" + str(pearsonr(df_f['avg_no_x'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pm10\" + \" \" + str(pearsonr(df_f['avg_pm'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pm2\" + \" \" + str(pearsonr(df_f['avg PM'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pres\" + \" \" + str(pearsonr(df_f['capped_pres'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"prec\" + \" \" + str(pearsonr(df_f['Prec'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"temp\" + \" \" + str(pearsonr(df_f['avg_temp'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Tair\" + \" \" + str(pearsonr(df_f['capped_Tair'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Twet\" + \" \" + str(pearsonr(df_f['capped_Twet'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Tdew\" + \" \" + str(pearsonr(df_f['capped_Tdew'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"RH\" + \" \" + str(pearsonr(df_f['capped_RH'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"trends\" + \" \" + str(pearsonr(df_f['capped_scale'],df_f['scaled data'])[0].round(decimals = 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f['capped_ozone'] = df_f['capped_ozone'].shift(-2)\n",
    "df_f['capped_bc(370)'] = df_f['capped_bc(370)'].shift(-1)\n",
    "df_f['capped_bc(880)'] = df_f['capped_bc(880)'].shift(-1)\n",
    "df_f['capped_wind'] = df_f['capped_wind'].shift(-2)\n",
    "df_f['UVI'] = df_f['UVI'].shift(-3)\n",
    "df_f['avg_no_y'] = df_f['avg_no_y'].shift(0)\n",
    "df_f['avg_no_x'] = df_f['avg_no_x'].shift(-1)\n",
    "df_f['avg_pm'] = df_f['avg_pm'].shift(-7)\n",
    "df_f['avg PM'] = df_f['avg PM'].shift(-3)\n",
    "df_f['capped_pres'] = df_f['capped_pres'].shift(-3)\n",
    "df_f['Prec'] = df_f['Prec'].shift(0)\n",
    "df_f['capped_scale'] = df_f['capped_scale'].shift(0)\n",
    "df_f['avg_temp'] = df_f['avg_temp'].shift(-5)\n",
    "df_f['capped_Tair'] = df_f['capped_Tair'].shift(-1)\n",
    "df_f['capped_Twet'] = df_f['capped_Twet'].shift(-1)\n",
    "df_f['capped_Tdew'] = df_f['capped_Tdew'].shift(-1)\n",
    "df_f['capped_RH'] = df_f['capped_RH'].shift(-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"oz\" + \" \" + str(pearsonr(df_f['capped_ozone'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"bc370\" + \" \" + str(pearsonr(df_f['capped_bc(370)'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"bc880\" + \" \" + str(pearsonr(df_f['capped_bc(880)'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"wind\" + \" \" + str(pearsonr(df_f['capped_wind'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"uvi\" + \" \" + str(pearsonr(df_f['UVI'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"no2\" + \" \" + str(pearsonr(df_f['avg_no_y'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"nox\" + \" \" + str(pearsonr(df_f['avg_no_x'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pm10\" + \" \" + str(pearsonr(df_f['avg_pm'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pm2\" + \" \" + str(pearsonr(df_f['avg PM'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"pres\" + \" \" + str(pearsonr(df_f['capped_pres'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"prec\" + \" \" + str(pearsonr(df_f['Prec'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"temp\" + \" \" + str(pearsonr(df_f['avg_temp'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Tair\" + \" \" + str(pearsonr(df_f['capped_Tair'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Twet\" + \" \" + str(pearsonr(df_f['capped_Twet'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"Tdew\" + \" \" + str(pearsonr(df_f['capped_Tdew'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"RH\" + \" \" + str(pearsonr(df_f['capped_RH'],df_f['scaled data'])[0].round(decimals = 4)))\n",
    "print(\"trends\" + \" \" + str(pearsonr(df_f['capped_scale'],df_f['scaled data'])[0].round(decimals = 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_f.to_csv('H:/asthma/data/asthma_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_f.drop(columns=['status', 'Date', 'scaled data', 'capped_Tair','capped_RH','avg_no_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_f['status'].values.reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.50, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_arima = Xtrain['Daily counts'].values.astype('float32')\n",
    "y_test_arima =  Xtest['Daily counts'].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.drop(columns = ['Daily counts'])\n",
    "Xtest = Xtest.drop(columns = ['Daily counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_order = (0, 1, 0) #(2,2,0)\n",
    "my_seasonal_order = (0, 1, 1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [x for x in y_train_arima]\n",
    "predictions = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "for t in range(len(y_test_arima)):\n",
    "     model = ARIMA(\n",
    "                history,\n",
    "                order=my_order)\n",
    "     model_fit = model.fit()\n",
    "     output = model_fit.forecast()\n",
    "     #print (output)\n",
    "     #predict_log.append(output[0]) \n",
    "     #yhat = 10**output[0]\n",
    "     predictions.append(int(output[0][0]))\n",
    "     obs = y_test_arima[t]\n",
    "     history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(predictions, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame()\n",
    "predicted_df[\"Predicted\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df['Predicted'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predicted_df.assign(status=pd.cut(predicted_df['Predicted'], \n",
    "                                bins = [-16.0, 10.0, 32.0],\n",
    "                             # bins=[-3, 0.3381293922380649 , 0.9290142083036281, 3], \n",
    "                              #  bins = [-1, 6, 8, 55],               \n",
    "                              # labels=['Low','Medium', 'High']\n",
    "                                labels = ['Low', 'High']                            \n",
    "                                                          \n",
    "                                                          ))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"status\"].replace({\"High\": 1, \"Low\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arima = pred_df[\"status\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, pred_arima))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, pred_arima, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, pred_arima, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, pred_arima, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "#df_f = sc.fit_transform(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sknet = MLPClassifier(hidden_layer_sizes=(50, ), learning_rate_init=0.001, max_iter=100, random_state=1, alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sknet.fit(Xtrain, ytrain.ravel())\n",
    "print(sknet.score(Xtrain, ytrain))\n",
    "#preds_train = sknet.predict(Xtrain)\n",
    "ypred = sknet.predict(Xtest)\n",
    "ypredp1 = sknet.predict_proba(Xtest)\n",
    "\n",
    "#print(ypred)\n",
    "#print(sknet.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, ypred))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, ypred, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, ypred, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, ypred, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=None, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(Xtrain, ytrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = clf.predict(Xtest)\n",
    "y_train_pred_rf = clf.predict(Xtrain)\n",
    "ypredp2 = clf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy\", metrics.accuracy_score(ytrain, y_train_pred_rf))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_rf))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_rf, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, y_pred_rf, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, y_pred_rf, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(kernel = \"rbf\", probability=True,  C=170, gamma= 3.0)\n",
    "clf_svm.fit(Xtrain,ytrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = clf_svm.predict(Xtest)\n",
    "ypredp2 = clf_svm.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_svm))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_svm, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, y_pred_svm, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, y_pred_svm, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clas = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 5, n_estimators = 1000, min_child_weight= 1, gamma = 0, nthread=4, seed = 27, feature_names=orig_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clas.fit(Xtrain, ytrain.ravel(), eval_metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_train = xg_clas.predict(Xtrain)\n",
    "y_pred_xgb = xg_clas.predict(Xtest)\n",
    "y_predprob_xgb_train = xg_clas.predict_proba(Xtrain)[:,1]\n",
    "y_predprob_xgb = xg_clas.predict_proba(Xtest)[:,1]\n",
    "ypredp3 = xg_clas.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_xgb))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, y_pred_xgb, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, y_pred_xgb, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, y_pred_xgb, average = \"weighted\"))\n",
    "print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(ytrain, y_predprob_xgb_train))\n",
    "print (\"AUC Score (Test): %f\" % metrics.roc_auc_score(ytest, y_predprob_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_clas)\n",
    "plt.rcParams['figure.figsize'] = plt.rcParamsDefault['figure.figsize']\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfeatures = X.columns\n",
    "dict_features = dict(enumerate(myfeatures))\n",
    "\n",
    "# feat importance with names f1,f2,...\n",
    "axsub = xgb.plot_importance(xg_clas)\n",
    "\n",
    "# get the original names back\n",
    "Text_yticklabels = list(axsub.get_yticklabels())\n",
    "dict_features = dict(enumerate(myfeatures))\n",
    "lst_yticklabels = [ Text_yticklabels[i].get_text().lstrip('f') for i in range(len(Text_yticklabels))]\n",
    "lst_yticklabels = [ dict_features[int(i)] for i in lst_yticklabels]\n",
    "\n",
    "axsub.set_yticklabels(lst_yticklabels)\n",
    "print(dict_features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain=xgb.DMatrix(Xtrain,label=ytrain)\n",
    "dtest=xgb.DMatrix(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters for xgboost\n",
    "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training our model \n",
    "num_round=8\n",
    "#from datetime import datetime \n",
    "#start = datetime.now() \n",
    "xg=xgb.train(parameters,dtrain, num_round) \n",
    "#stop = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_xg2=xg.predict(dtest) \n",
    "ypred_xg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(ypred_xg2 >= 0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,549): \n",
    "    if ypred_xg2[i]>= 0.52:       # setting threshold to .5 \n",
    "       ypred_xg2[i]=1 \n",
    "    else: \n",
    "       ypred_xg2[i]=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, ypred_xg2))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, ypred_xg2, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, ypred_xg2, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, ypred_xg2, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_array = np.empty(549, int)\n",
    "for i in range(len(y_pred_xgb)):\n",
    "    if((ypred_xg2[i] == y_pred_xgb[i] ==1) or (ypred_xg2[i] == pred_arima[i] ==1) or (pred_arima[i] == y_pred_xgb[i] ==1)):\n",
    "        emp_array[i] = 1\n",
    "        continue\n",
    "    else:\n",
    "        emp_array[i] = 0\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(ytest, emp_array))\n",
    "print(\"Accuracy\", metrics.accuracy_score(ytest, emp_array, normalize = False))\n",
    "print(\"Precision\", metrics.precision_score(ytest, emp_array, average = \"weighted\"))\n",
    "print(\"Recall\", metrics.recall_score(ytest, emp_array, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(ytest, emp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
